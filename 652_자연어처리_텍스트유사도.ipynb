{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNDmD1Nfk8PYAXmwCVr/Bk8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/onleey/Machine-Learning_demo/blob/master/652_%EC%9E%90%EC%97%B0%EC%96%B4%EC%B2%98%EB%A6%AC_%ED%85%8D%EC%8A%A4%ED%8A%B8%EC%9C%A0%EC%82%AC%EB%8F%84.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 텍스트 유사도\n",
        "- 임베딩으로 각 단어들의 벡터를 구한 다음 벡터 간의 거리를 계산하는 방법으로 단어 간의 의미가 얼마나 유사한지 계산할 수 있다. 문장 역시 단어들의 묶음이기 때문에 하나의 벡터로 묶어서 문장간의 유사도를 계산할 수 있다.\n",
        "- 두 문장 간의 유사도를 계산하기 위해서는 문장 내에 존재하는 단어들을 수치화해야 한다.  이때 언어 모델에 따라 통계를 이용하는 방법과 인공 신경망을 이용하는 방법으로 나눌 수 있다. Word2Vec은 인공 신경망을 이용이고 n-gram 유사도와코사인 유사도는 통계적방식이다."
      ],
      "metadata": {
        "id": "3bFn3ve3COnb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. n-gram 유사도\n",
        "\n",
        "- n-gram은 주어진 문장에서 n개의 연속적인 단어 시퀀스(단어 나열)를 의미한다. n-gram은 문장에서 n개의 단어를 토큰으로 사용한다. 이는 이웃한 단어의 출현 횟수를 통계적으로 표현해 텍스트의 유사도를 계산하는 방법이다.\n",
        "- 서로 다른 문장을 n-gram으로 비교하면 단어의 출현 빈도에 기반한 유사도를 계산할 수 있으며, 이를 통해 논문 인용이나 도용 정도를 조사할 수 있다.\n",
        "- https://medium.com/@pankajchandravanshi/nlp-unlocked-n-grams-006-ceab1bc56bf4\n",
        "- n이 1인 경우 1- gram 또는 유니그램(unigram), 2인 경우 2-gram 또는 바이그램(bigram), 3인 경우 3-gram 또는 트라이그램(trigram), 4이상은 숫자만 앞쪽에 붙여 부른다.\n",
        "- n-gram 유사도  \n",
        "  similarity = $\\frac{tf(A,B)}{tokens(A)}$\n",
        "   - tf(term frequency)는 두 문장 A와 B에서 동일한 토큰의 출현 빈도를 뜻하며, tokens는 해당 문장에서 전체 토큰 수를 의미한다. 여기서 토큰이란 n-gram으로 분리된 단어이다. 1.0에 가까울수록 B가 A에 유사하다고 볼 수 있다.\n",
        "   - 문장 A의 2-gram 토큰   \n",
        "   A : 6월에 뉴턴은 선생님의 제안으로 트리니티에 입학했다.   \n",
        "   B : 6월에 뉴턴은 선생님의 제안으로 대학교에 입학했다.    \n",
        "\n",
        "\n",
        "   | |(6월,뉴턴)|(뉴턴,선생님)|(선생님,제안)|(제안,트리니티)|(트리니티,입학)|(입학)| | |\n",
        "   |---|---|---|---|---|---|---|---|---|\n",
        "   |A|1|1|1|1|1|1|6|=>toekns(A)|\n",
        "   |B|1|1|1|0|0|1|4|=>tf(A,B)|"
      ],
      "metadata": {
        "id": "67jLZRccCdlH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cbafU52FBF6a",
        "outputId": "478bdfe7-e0b7-4dd1-c9ef-b902871afb87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting konlpy\n",
            "  Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.4/19.4 MB\u001b[0m \u001b[31m67.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting JPype1>=0.7.0 (from konlpy)\n",
            "  Downloading JPype1-1.4.1-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (465 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m465.3/465.3 kB\u001b[0m \u001b[31m27.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from konlpy) (4.9.3)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.10/dist-packages (from konlpy) (1.23.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from JPype1>=0.7.0->konlpy) (23.2)\n",
            "Installing collected packages: JPype1, konlpy\n",
            "Successfully installed JPype1-1.4.1 konlpy-0.6.0\n"
          ]
        }
      ],
      "source": [
        "!pip install konlpy"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "n-gram 유사도"
      ],
      "metadata": {
        "id": "KQK6lezwDWGq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from konlpy.tag import Komoran\n",
        "\n",
        "\n",
        "# 어절 단위 n-gram\n",
        "def word_ngram(bow, num_gram):\n",
        "    text = tuple(bow)\n",
        "    ngrams = [text[x:x + num_gram] for x in range(0, len(text))]\n",
        "    return tuple(ngrams)\n",
        "\n",
        "\n",
        "# 음절 n-gram 분석\n",
        "def phoneme_ngram(bow, num_gram):\n",
        "    sentence = ' '.join(bow)\n",
        "    text = tuple(sentence)\n",
        "    slen = len(text)\n",
        "    ngrams = [text[x:x + num_gram] for x in range(0, slen)]\n",
        "    return ngrams\n",
        "\n",
        "\n",
        "# 유사도 계산\n",
        "def similarity(doc1, doc2):\n",
        "    cnt = 0\n",
        "    for token in doc1:\n",
        "        if token in doc2:\n",
        "            cnt = cnt + 1\n",
        "\n",
        "    return cnt/len(doc1)\n",
        "\n",
        "\n",
        "sentence1 = '6월에 뉴턴은 선생님의 제안으로 트리니티에 입학하였다'\n",
        "sentence2 = '6월에 뉴턴은 선생님의 제안으로 대학교에 입학하였다'\n",
        "sentence3 = '나는 맛잇는 밥을 뉴턴 선생님과 함께 먹었습니다.'\n",
        "\n",
        "komoran = Komoran()\n",
        "bow1 = komoran.nouns(sentence1)\n",
        "bow2 = komoran.nouns(sentence2)\n",
        "bow3 = komoran.nouns(sentence3)\n",
        "\n",
        "print(bow1)\n",
        "print(bow2)\n",
        "print(bow3)\n",
        "\n",
        "doc1 = word_ngram(bow1, 2)\n",
        "doc2 = word_ngram(bow2, 2)\n",
        "doc3 = word_ngram(bow3, 2)\n",
        "\n",
        "print(doc1)\n",
        "print(doc2)\n",
        "print(doc3)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B-LSuLCADGk_",
        "outputId": "6a7427e2-d49b-4f9b-e8a7-382f14aad466"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['6월', '뉴턴', '선생님', '제안', '트리니티', '입학']\n",
            "['6월', '뉴턴', '선생님', '제안', '대학교', '입학']\n",
            "['맛', '밥', '뉴턴', '선생', '님과 함께']\n",
            "(('6월', '뉴턴'), ('뉴턴', '선생님'), ('선생님', '제안'), ('제안', '트리니티'), ('트리니티', '입학'), ('입학',))\n",
            "(('6월', '뉴턴'), ('뉴턴', '선생님'), ('선생님', '제안'), ('제안', '대학교'), ('대학교', '입학'), ('입학',))\n",
            "(('맛', '밥'), ('밥', '뉴턴'), ('뉴턴', '선생'), ('선생', '님과 함께'), ('님과 함께',))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from numpy import dot\n",
        "a = np.array([[1,2,3]])\n",
        "\n",
        "#행렬곱\n",
        "#print(dot(a,a))\n",
        "\n",
        "#행렬내적\n",
        "print(a*a)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S-sXgWWvMj8Y",
        "outputId": "aba5c0e1-a9f0-4214-b4bb-4232fc7582ad"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1 4 9]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. 코사인 유사도\n",
        "\n",
        "- 단어나 문장을 벡터로 표현할 수 있다면 벡터 간 거리나 각도를 이용해 유사성을 파악할 수 있다. 벡터 간 거리를 구하는 방법은 다양하며 여기에서는 코사인 유사도를 이용한다.\n",
        "- 코사인 유사도는 두 벡터 간 코사인 각도를 이용해 유사도를 측정하는 방법이다. 일반적으로 코사인 유사도는 벡터의 크기가 중요하지 않을 때 그 거리를 측정하기 위해 사용한다.\n",
        "- 단어의 출현 빈도를 통해 유사도 계산을 한다면 동일한 단어가 많이 포함되어 있을수록 벡터의 크기가 커진다. 이때 코사인 유사도는 벡터의 크기와 상관없이 결과가 안정적이다.\n",
        "- n-gram의 경우 동일한 단어가 문서 내에 자주 등장하면 유사도 결과에 안 좋은 영향을 미칠 수 밖에 없다. 코사인 유사도는 다양한 차원에서 적용 가능해 실무에서 많이 사용한다.\n",
        "- 코사인 -1 ~ 1 사이의 값을 가지며, 두 벡터의 방향이 완전히 동일한 경우에는 1, 반대 방향인 경우에는 -1, 두 벡터가 서로 직각을 이루면 0의 값을 가진다. 즉, 두 벡터의 방향이 같아 질수록 유사하다 볼 수 있다.\n",
        "- 코사인 유사도는 공간 벡터의 내적과 크기를 이용해 코사인 각도를 계산한다.\n",
        "\n",
        "similarity = $\\cos$θ = $\\frac{A·B}{||A||||B||}$ = $\\frac{\\sum_{i=1}^{n}A_i𝚇  B_i}{\\sqrt{\\sum_{i=1}^{n}}(A_{i})^2 X \\sqrt{\\sum_{i=1}^{n}}(B_{i})^2}$\n",
        "\n",
        "\n",
        "A : 6월에 뉴턴은 선생님의 제안으로 트리니티에 입학했다.   \n",
        "   B : 6월에 뉴턴은 선생님의 제안으로 대학교에 입학했다.    \n",
        "\n",
        "\n",
        "   | |6월|뉴턴|선생님|제안|트리니티|입학|대학 |\n",
        "   |---|---|---|---|---|---|---|---|\n",
        "   |A|1|1|1|1|1|1|0|\n",
        "   |B|1|1|1|1|0|1|1|\n",
        "\n",
        "\n",
        "   ${A·B}$ = $\\sum_{i=1}^{n}A_i 𝚇  B_i$  \n",
        "            = (1*1) + (1*1)+ (1*1)+ (1*1)+ (1*0)+ (1*1)+ (0*1)  \n",
        "            = 1+1+1+1+0+1+0  \n",
        "            = 5  \n",
        "\n",
        "   \n",
        "  ${||A||||B||}$  = ${\\sqrt{\\sum_{i=1}^{n}}(A_{i})^2 X \\sqrt{\\sum_{i=1}^{n}}(B_{i})^2}$     \n",
        "= $\\sqrt {1^{2}+1^{2}+1^{2}+1^{2}+1^{2}+1^{2}+0^{2}} X \\sqrt {1^{2}+1^{2}+1^{2}+1^{2}+0^{2}+1^{2}+1^{2}}$    \n",
        "= $\\sqrt{6} X \\sqrt{6}$  \n",
        "= $\\sqrt{36}$  \n",
        "= 6  \n",
        "\n",
        "\n",
        "similarity = $\\cos$θ = $\\frac{A·B}{||A||||B||}$ = $\\frac{5}{6}$ = 0.83333"
      ],
      "metadata": {
        "id": "p_6oFZ5uPeHu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from konlpy.tag import Komoran\n",
        "import numpy as np\n",
        "from numpy import dot\n",
        "from numpy.linalg import norm\n",
        "\n",
        "\n",
        "# 코사인 유사도 계산\n",
        "def cos_sim(vec1, vec2):\n",
        "    return dot(vec1, vec2) / (norm(vec1) * norm(vec2))\n",
        "\n",
        "\n",
        "# TDM 만들기\n",
        "def make_term_doc_mat(sentence_bow, word_dics):\n",
        "    freq_mat = {}\n",
        "\n",
        "    for word in word_dics:\n",
        "        freq_mat[word] = 0\n",
        "\n",
        "    for word in word_dics:\n",
        "        if word in sentence_bow:\n",
        "            freq_mat[word] += 1\n",
        "\n",
        "    return freq_mat"
      ],
      "metadata": {
        "id": "DgG30TuhSCeF"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 단어 벡터 만들기\n",
        "def make_vector(tdm):\n",
        "    vec = []\n",
        "    for key in tdm:\n",
        "        vec.append(tdm[key])\n",
        "    return vec\n",
        "\n",
        "\n",
        "# 문장 정의\n",
        "sentence1 = '6월에 뉴턴은 선생님의 제안으로 트리니티에 입학하였다'\n",
        "sentence2 = '6월에 뉴턴은 선생님의 제안으로 대학교에 입학하였다'\n",
        "sentence3 = '나는 맛잇는 밥을 뉴턴 선생님과 함께 먹었습니다.'\n",
        "\n",
        "# 헝태소분석기를 이용해 단어 묶음 리스트 생성\n",
        "komoran = Komoran()\n",
        "bow1 = komoran.nouns(sentence1)\n",
        "bow2 = komoran.nouns(sentence2)\n",
        "bow3 = komoran.nouns(sentence3)\n",
        "\n",
        "# 단어 묶음 리스트를 하나로 합침\n",
        "bow = bow1 + bow2 + bow3\n",
        "\n",
        "# 단어 묶음에서 중복제거해 단어 사전 구축\n",
        "word_dics = []\n",
        "for token in bow:\n",
        "    if token not in word_dics:\n",
        "        word_dics.append(token)\n",
        "\n",
        "\n",
        "# 문장 별 단어 문서 행렬 계산\n",
        "freq_list1 = make_term_doc_mat(bow1, word_dics)\n",
        "freq_list2 = make_term_doc_mat(bow2, word_dics)\n",
        "freq_list3 = make_term_doc_mat(bow3, word_dics)\n",
        "print(freq_list1)\n",
        "print(freq_list2)\n",
        "print(freq_list3)\n",
        "\n",
        "\n",
        "\n",
        "# 코사인 유사도 계산\n",
        "doc1 = np.array(make_vector(freq_list1))\n",
        "doc2 = np.array(make_vector(freq_list2))\n",
        "doc3 = np.array(make_vector(freq_list3))\n",
        "r1 = cos_sim(doc1, doc2)\n",
        "r2 = cos_sim(doc3, doc1)\n",
        "print(r1)\n",
        "print(r2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nl7F1SuSSE70",
        "outputId": "1139e1ba-8857-43d8-b7ff-b9d8ec8a0081"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'6월': 1, '뉴턴': 1, '선생님': 1, '제안': 1, '트리니티': 1, '입학': 1, '대학교': 0, '맛': 0, '밥': 0, '선생': 0, '님과 함께': 0}\n",
            "{'6월': 1, '뉴턴': 1, '선생님': 1, '제안': 1, '트리니티': 0, '입학': 1, '대학교': 1, '맛': 0, '밥': 0, '선생': 0, '님과 함께': 0}\n",
            "{'6월': 0, '뉴턴': 1, '선생님': 0, '제안': 0, '트리니티': 0, '입학': 0, '대학교': 0, '맛': 1, '밥': 1, '선생': 1, '님과 함께': 1}\n"
          ]
        }
      ]
    }
  ]
}