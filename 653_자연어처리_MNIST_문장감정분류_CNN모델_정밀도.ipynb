{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNRCdRyPvkJ3vI7K0CBgPpX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/onleey/Machine-Learning_demo/blob/master/653_%EC%9E%90%EC%97%B0%EC%96%B4%EC%B2%98%EB%A6%AC_MNIST_%EB%AC%B8%EC%9E%A5%EA%B0%90%EC%A0%95%EB%B6%84%EB%A5%98_CNN%EB%AA%A8%EB%8D%B8_%EC%A0%95%EB%B0%80%EB%8F%84.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oG3AZAAtjiXA",
        "outputId": "2738fb4f-be86-45d1-863e-17a6d0ef9442"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/MyDrive/001\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "%cd '/content/drive/MyDrive/001'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "tf.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "QZT0Q0E_jpgd",
        "outputId": "e450b233-073b-4276-c03a-7f58190eb886"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.15.0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "from keras.datasets import mnist"
      ],
      "metadata": {
        "id": "r76jNb_jjqFV"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dir(keras.datasets))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XlABxIE5jqIE",
        "outputId": "2060dba1-e036-41d1-adb9-0a304dac4b2e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', 'boston_housing', 'cifar10', 'cifar100', 'fashion_mnist', 'imdb', 'mnist', 'reuters']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 필요한 모듈 임포트\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow import keras\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Flatten, Dense\n",
        "\n",
        "#MNIST 데이터셋 가지고 오기\n",
        "(x_train, y_train), (x_test,y_test) = mnist.load_data()\n",
        "x_train, x_test = x_train/ 255.0, x_test / 255.0 #데이터 정규화\n",
        "#MNIST의 픽셀값 범위가 0~255이기 때문에 모든 수를 1미만으로 정규화 시키기 위해 255.0을 나누어 준다.\n",
        "#print(x_train.shape) #(60000, 28, 28)\n",
        "#plt.imshow(x_train[7211])\n",
        "\n",
        "#tf.data를 사용하여 데이터셋을 섞고 배치 만들기\n",
        "ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(10000)\n",
        "train_size = int(len(x_train)*0.7) #학습 세트 : 검증세트 = 7:3\n",
        "\n",
        "#batch() > 처리하려고 들고오는 데이터의 수 (괄호안에 숫자만큼 가지고와서 데이터 처리)\n",
        "train_ds = ds.take(train_size).batch(20)\n",
        "val_ds = ds.skip(train_size).batch(20)\n",
        "\n",
        "########################\n",
        "#MNIST 분류 모델 구성###\n",
        "########################\n",
        "\n",
        "#Sequential() : 순차적인 신경망 구성할 때 사용되는 클래스\n",
        "model = Sequential()\n",
        "#MNIST 이미지의 크기가 28*28픽셀로 구성되어있는데, flatten을 이용해서 1차원으로 펼쳐준다.\n",
        "model.add(Flatten(input_shape=(28,28))) #input layer// 인풋층\n",
        "model.add(Dense(20, activation='relu')) #hidden layer// 히든층\n",
        "model.add(Dense(20, activation='relu')) #hidden layer// 히든층\n",
        "model.add(Dense(10, activation='softmax')) #output layer // 아웃풋층\n",
        "\n",
        "#############\n",
        "##모델 생성##(알고리즘 생성)\n",
        "#############\n",
        "\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
        "#model.complie(loss=categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
        "\n",
        "#실제값과 예측값의 차이를 구하는 손실함수를 구할때 //  정해져있음 그냥 공식임// 가져다 쓰면됨\n",
        "#optimizer => 경사하방법을 구하는 방법 중 하나 지정\n",
        "#class(정답)가 3개 이상일때 ==> loss='sparse_categorical_crossentropy'\n",
        "#class(정답)가 2개 이하일때 ==> loss=categorical_crossentropy'\n",
        "\n",
        "#모델(알고리즘) 학습\n",
        "hist = model.fit(train_ds, validation_data=val_ds, epochs=10)\n",
        "\n",
        "#모델 평가\n",
        "print('모델 평가')\n",
        "model.evaluate(x_test, y_test)\n",
        "\n",
        "#모델 정보 출력\n",
        "model.summary()\n",
        "\n",
        "#모델 저장\n",
        "model.save('mnist_model.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wfgn0I2wjqKk",
        "outputId": "c36a3881-456f-420a-e598-7bea89801eb2"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n",
            "Epoch 1/10\n",
            "2100/2100 [==============================] - 8s 3ms/step - loss: 0.7526 - accuracy: 0.7854 - val_loss: 0.3606 - val_accuracy: 0.8946\n",
            "Epoch 2/10\n",
            "2100/2100 [==============================] - 5s 2ms/step - loss: 0.3425 - accuracy: 0.9015 - val_loss: 0.2932 - val_accuracy: 0.9158\n",
            "Epoch 3/10\n",
            "2100/2100 [==============================] - 6s 3ms/step - loss: 0.2934 - accuracy: 0.9162 - val_loss: 0.2645 - val_accuracy: 0.9227\n",
            "Epoch 4/10\n",
            "2100/2100 [==============================] - 7s 3ms/step - loss: 0.2652 - accuracy: 0.9252 - val_loss: 0.2384 - val_accuracy: 0.9311\n",
            "Epoch 5/10\n",
            "2100/2100 [==============================] - 7s 3ms/step - loss: 0.2428 - accuracy: 0.9307 - val_loss: 0.2253 - val_accuracy: 0.9354\n",
            "Epoch 6/10\n",
            "2100/2100 [==============================] - 10s 5ms/step - loss: 0.2282 - accuracy: 0.9338 - val_loss: 0.2201 - val_accuracy: 0.9353\n",
            "Epoch 7/10\n",
            "2100/2100 [==============================] - 5s 2ms/step - loss: 0.2105 - accuracy: 0.9391 - val_loss: 0.2093 - val_accuracy: 0.9407\n",
            "Epoch 8/10\n",
            "2100/2100 [==============================] - 7s 3ms/step - loss: 0.2021 - accuracy: 0.9423 - val_loss: 0.1978 - val_accuracy: 0.9434\n",
            "Epoch 9/10\n",
            "2100/2100 [==============================] - 7s 3ms/step - loss: 0.1915 - accuracy: 0.9445 - val_loss: 0.1955 - val_accuracy: 0.9446\n",
            "Epoch 10/10\n",
            "2100/2100 [==============================] - 5s 2ms/step - loss: 0.1820 - accuracy: 0.9465 - val_loss: 0.1813 - val_accuracy: 0.9471\n",
            "모델 평가\n",
            "313/313 [==============================] - 2s 4ms/step - loss: 0.1894 - accuracy: 0.9461\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten (Flatten)           (None, 784)               0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 20)                15700     \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 20)                420       \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 10)                210       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 16330 (63.79 KB)\n",
            "Trainable params: 16330 (63.79 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##문장 감정 분류 CNN 모델"
      ],
      "metadata": {
        "id": "H_8slc33j4WE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 필요한 모듈 임포트\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import preprocessing\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Embedding, Dense, Dropout, Conv1D, GlobalMaxPool1D, concatenate\n",
        "\n",
        "# 데이터 읽어오기\n",
        "train_file = \"./datasets/ratings3.csv\"\n",
        "data = pd.read_csv(train_file, delimiter=',')\n",
        "features = data['Q'].tolist()\n",
        "labels = data['label'].tolist()\n",
        "\n",
        "print(data.head())\n",
        "print(data['label'].value_counts()) #0:일상다반사 // 1:이별(부정) //2.사랑(긍정)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        },
        "id": "I3bHHktJjqM8",
        "outputId": "0bdeeca2-31be-47e3-e2e8-21f17a10f895"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ParserError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-d86191ad8965>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# 데이터 읽어오기\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mtrain_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"./datasets/ratings3.csv\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Q'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 )\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1776\u001b[0m                     \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m                     \u001b[0mcol_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1778\u001b[0;31m                 \u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1779\u001b[0m                     \u001b[0mnrows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1780\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    228\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlow_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m                 \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_low_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m                 \u001b[0;31m# destructive to chunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_concatenate_chunks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: Expected 2 fields in line 27, saw 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###CNN(Convolutional neural network, 합성곱 신경망)\n",
        " - 시각적 이미지 분석 및 분류에 일반적으로 사용되는 인공신경망이다."
      ],
      "metadata": {
        "id": "ezHNpFYWj6_k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 단어 인덱스 시퀀스 벡터\n",
        "corpus = [preprocessing.text.text_to_word_sequence(text) for text in features]\n",
        "print(corpus)"
      ],
      "metadata": {
        "id": "OHDf_W7ejqPE"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import OrderedDict\n",
        "\n",
        "tokenizer = preprocessing.text.Tokenizer()\n",
        "print(dir(tokenizer))\n",
        "tokenizer.fit_on_texts(corpus)\n",
        "\n",
        "#문장내 모든 단어를 수퀀스 번호로 변환한다.\n",
        "sequences = tokenizer.texts_to_sequences(corpus)\n",
        "print(sequences)\n",
        "word_index = tokenizer.word_index\n",
        "print(word_index)\n",
        "\n",
        "MAX_SEQ_LEN = 15 #단어 시퀀스 벡터 크기\n",
        "padded_seqs = preprocessing.sequence.pad_sequences(sequences, maxlen=MAX_SEQ_LEN, padding='post')\n",
        "print(padded_seqs)\n",
        "print(padded_seqs[0:1])\n",
        "\n",
        "#학습용,검증용,테스트용 데이터 셋 생성\n",
        "#학습용,검즘용.테스트용 = 7:2:1\n",
        "\n",
        "ds = tf.data.Dataset.from_tensor_slices((padded_seqs, labels))\n",
        "ds = ds.shuffle(len(features))\n",
        "\n",
        "train_size = int(len(padded_seqs)*0.7)\n",
        "val_size = int(len(padded_seqs)*0.2)\n",
        "test_size = int(len(padded_seqs)*0.1)\n",
        "\n",
        "train_ds = ds.take(train_size).batch(20)\n",
        "val_ds = ds.skip(train_size).take(val_size).batch(20)\n",
        "test_ds= ds.skip(train_size+val_size).take(test_size).batch(20)\n",
        "\n",
        "#하이퍼파라미터\n",
        "dropout_prob = 0.5 #결과보고 과적합 비율 조절 필요\n",
        "EMB_SIZE = 128\n",
        "EPOCH = 35\n",
        "VOCAB_SIZE = len(word_index) +1 #전체 단어 수\n",
        "\n",
        "#CNN모델 정의\n",
        "input_layer = Input(shape=(MAX_SEQ_LEN))\n",
        "print('input_layer:', input_layer.shape)\n",
        "\n",
        "\n",
        "#임베딩 크기 생성\n",
        "embedding_layer= Embedding(VOCAB_SIZE, EMB_SIZE, input_length=MAX_SEQ_LEN)(input_layer)\n",
        "print('embedding_layer:', embedding_layer.shape) #embedding_layer: (None, 15, 128)\n",
        "dropout_emb = Dropout(rate=dropout_prob)(embedding_layer)\n",
        "print('dropout_emb:', dropout_emb.shape)         #dropout_emb: (None, 15, 128)\n",
        "\n",
        "#CNN tensor 사이즈 계산하기 : (이미지 크기 - kernel_size)/strides + 1\n",
        "#풀링 연산이란 합성곱 연산결과로 나온 특징맵의 크기를 줄이거나 주요한 특징을 추출하기 위해서 사용되는 연산이다.\n",
        "#풀링 연산에는 최대풀링과 평균풀링 연산이 있는데 주로 최대 풀링 연산을 사용한다.\n",
        "#풀링 연산에도 합성곱연산에서 사용되는 윈도우 크기, 스트라이드, 패딩 개념등이 동일하게 적용된다.\n",
        "conv1 = Conv1D(filters = 128, kernel_size=3, padding='valid', activation=tf.nn.relu)(dropout_emb)\n",
        "print('conv1:', conv1.shape) #conv1: (None, 13, 128)\n",
        "#filter를 처리하면 셀의 크기가 -2되는데, 원래 사이즈 데이터를 가지고싶다면 padding='same'을 주면된다.\n",
        "pool1 = GlobalMaxPool1D()(conv1)\n",
        "print('pool1:',pool1.shape) #pool1: (None, 128)\n",
        "conv2 = Conv1D(filters = 128, kernel_size=4, padding='valid', activation=tf.nn.relu)(dropout_emb)\n",
        "print('conv2:', conv2.shape) #conv2: (None, 12, 128)\n",
        "pool2 = GlobalMaxPool1D()(conv2)\n",
        "print('pool2:',pool2.shape) #pool2: (None, 128)\n",
        "conv3 = Conv1D(filters = 128, kernel_size=5, padding='valid', activation=tf.nn.relu)(dropout_emb)\n",
        "print('conv3:', conv3.shape) #conv3: (None, 11, 128)\n",
        "pool3 = GlobalMaxPool1D()(conv3)\n",
        "print('pool3:',pool3.shape) #pool3: (None, 128)\n",
        "\n",
        "\n",
        "#3,4,5 - gram 이후 합치기\n",
        "concat = concatenate([pool1, pool2, pool3])\n",
        "print('concat:', concat.shape) #concat: (None, 384)\n",
        "\n",
        "hidden = Dense(128, activation=tf.nn.relu)(concat)  #relu - 10보다 작으면 0을 / 그 이상이면 그대로 값 반환\n",
        "dropout_hidden = Dropout(rate=dropout_prob)(hidden) #과적합(overfitting)방지를 위한 dropout\n",
        "logits = Dense(3, name='logits')(dropout_hidden)\n",
        "print('logits:', logits.shape) #logits : (None, 3)\n",
        "predictions = Dense(3, activation=tf.nn.softmax)(logits)\n",
        "print('predictions:', predictions.shape) #predictions: (None,3)\n",
        "\n",
        "#모델생성\n",
        "model = Model(inputs=input_layer, outputs=predictions)\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "#모델 학습\n",
        "model.fit(train_ds, validation_data=val_ds, epochs=EPOCH)\n",
        "\n",
        "#모델 평가\n",
        "loss, accuracy = model.evaluate(test_ds, verbose=1)\n",
        "print('Accuracy: %f' % (accuracy * 100))\n",
        "print('loss: %f'% (loss))\n",
        "\n",
        "model.save('./source/cnn_model.h5')"
      ],
      "metadata": {
        "id": "BqCfwx1-jqRc"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "** 문답 데이터  **"
      ],
      "metadata": {
        "id": "-LussretkBYU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "from tensorflow import keras\n",
        "from keras.models import Model, load_model\n",
        "from keras import preprocessing\n",
        "\n",
        "# 데이터 읽어오기\n",
        "train_file = \"./datasets/ratings3.csv\"\n",
        "data = pd.read_csv(train_file, delimiter=',')\n",
        "features = data['Q'].tolist()\n",
        "labels = data['label'].tolist()\n",
        "\n",
        "# 단어 인덱스 시퀀스 벡터\n",
        "corpus = [preprocessing.text.text_to_word_sequence(text) for text in features]\n",
        "tokenizer = preprocessing.text.Tokenizer()\n",
        "tokenizer.fit_on_texts(corpus)\n",
        "sequences = tokenizer.texts_to_sequences(corpus)\n",
        "MAX_SEQ_LEN = 15 # 단어 시퀀스 벡터 크기\n",
        "padded_seqs = preprocessing.sequence.pad_sequences(sequences, maxlen=MAX_SEQ_LEN, padding='post')\n",
        "\n",
        "# 테스트용 데이터셋 생성\n",
        "ds = tf.data.Dataset.from_tensor_slices((padded_seqs, labels))\n",
        "ds = ds.shuffle(len(features))\n",
        "test_ds = ds.take(2000).batch(20) # 테스트 데이터셋\n",
        "\n",
        "# 감정 분류 CNN 모델 불러오기\n",
        "model = load_model('cnn_model.h5')\n",
        "model.summary()\n",
        "model.evaluate(test_ds, verbose=2)\n",
        "\n",
        "# 테스트용 데이터셋의 10212번째 데이터 출력\n",
        "print(\"단어 시퀀스 : \", corpus[10212])\n",
        "print(\"단어 인덱스 시퀀스 : \", padded_seqs[10212])\n",
        "print(\"문장 분류(정답) : \", labels[10212])\n",
        "\n",
        "# 테스트용 데이터셋의 10212번째 데이터 감정 예측\n",
        "picks = [10212]\n",
        "predict = model.predict(padded_seqs[picks])\n",
        "predict_class = tf.math.argmax(predict, axis=1)\n",
        "print(\"감정 예측 점수 : \", predict)\n",
        "print(\"감정 예측 클래스 : \", predict_class.numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        },
        "id": "Ogs13aphjqTl",
        "outputId": "17021883-f6b8-4a38-995f-9b0e07f7d0a5"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ParserError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-690366dfa505>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# 데이터 읽어오기\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mtrain_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"./datasets/ratings3.csv\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Q'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 )\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1776\u001b[0m                     \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m                     \u001b[0mcol_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1778\u001b[0;31m                 \u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1779\u001b[0m                     \u001b[0mnrows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1780\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    228\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlow_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m                 \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_low_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m                 \u001b[0;31m# destructive to chunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_concatenate_chunks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: Expected 2 fields in line 27, saw 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import preprocessing\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Embedding, Dense, Dropout, Conv1D, GlobalAveragePooling1D, concatenate\n",
        "\n",
        "#데이터 읽어오기\n",
        "train_file = './datasets/chatbot_data.csv'\n",
        "data = pd.read_csv(train_file, delimiter=',')\n",
        "features = data['Q'].tolist()\n",
        "labels = data['label'].tolist()\n",
        "\n",
        "\n",
        "#d단어 인덱스 시퀀스벡터\n",
        "corpus = [preprocessing.text.text_to_word_sequence(text)for text in features]\n",
        "print('corpus:',corpus)\n",
        "\n",
        "#\n",
        "tokenizer = preprocessing.text.Tokenizer()\n",
        "tokenizer.fit_on_texts(corpus)\n",
        "sequences = tokenizer.texts_to_sequences(corpus)\n",
        "word_index = tokenizer.word_index\n",
        "MAX_SEQ_LEN =15\n",
        "padded_seqs= preprocessing.sequence.pad_sequences(sequences, maxlen=MAX_SEQ_LEN, padding='post')\n",
        "\n",
        "#학습용 검증용 테스트요 ㅇ데이터셋 생성\n",
        "ds= tf.data.Dataset.from_tensor_slices((padded_seqs, labels))\n",
        "ds = ds.shuffle(len(features))\n",
        "train_size = int(len(padded_seqs)*0.7)\n",
        "val_size =int(len(padded_seqs)*0.2)\n",
        "test_size =int(len(padded_seqs)*0.1)\n",
        "train_ds = ds.take(train_size).batch(20)\n",
        "val_ds = ds.skip(train_size).take(val_size).batch(20)\n",
        "test_ds = ds.skip(train_size +val_size).take(test_size).batch(20)\n",
        "\n",
        "#하이퍼 파라미터 설정\n",
        "dropout_prob = 0.5\n",
        "EMB_SIZE = 128\n",
        "EPOCH = 10\n",
        "VOCAB_SIZE = len(word_index)+1\n",
        "\n",
        "##CNN 모델정의\n",
        "input_layer = Input(shape=(MAX_SEQ_LEN,))\n",
        "print('input_layer:', input_layer.shape)\n",
        "\n",
        "embedding_layer = Embedding(VOCAB_SIZE, EMB_SIZE, input_length=MAX_SEQ_LEN)(input_layer)\n",
        "print('embedding_later: ', embedding_layer.shape)\n",
        "dropout_emb = Dropout(rate =dropout_prob)(embedding_layer)\n",
        "print('dropout_emb:',dropout_emb.shape)\n",
        "\n",
        "#CNN tensor sieze 계산하기\n",
        "\n",
        "conv1= Conv1D(filters= 128, kernel_size=3 , padding ='valid', strides=1, activation=tf.nn.relu)(dropout_emb)\n",
        "print('conv1:', conv1.shape)\n",
        "Pool1 = GlobalMaxPool1D() (conv1)\n",
        "print('pool1:', pool1.shape)\n",
        "\n",
        "\n",
        "conv2= Conv1D(filters= 128, kernel_size=3 , padding ='valid', activation=tf.nn.relu)(dropout_emb)\n",
        "print('conv1:', conv2.shape)\n",
        "Pool2 = GlobalMaxPool1D() (conv2)\n",
        "print('pool2:', pool2.shape)\n",
        "\n",
        "conv3= Conv1D(filters= 128, kernel_size=3 , padding ='valid', activation=tf.nn.relu)(dropout_emb)\n",
        "print('conv1:', conv3.shape)\n",
        "Pool3 = GlobalMaxPool1D() (conv3)\n",
        "print('pool3:', pool3.shape)\n",
        "\n",
        "\n",
        "# 3,4,5 - gram 이후 합치기\n",
        "concat = concatenate([pool1,pool2,pool3])\n",
        "print('concat:', concat.shape)\n",
        "\n",
        "hidden = Dense(128, activation = tf.nn.relu)(concat)\n",
        "print('hidden:', hidden.shape)\n",
        "\n",
        "dropout_hidden = Dropout(rate = dropout_prob)(hidden)\n",
        "logits = Dense(3,name = 'logits')(dropout_hidden)\n",
        "print('logits:', logits.shape)\n",
        "predictions= Dense(3, activation = tf.nn.softmax)(logits)\n",
        "print('predictions:', predictions.shape)\n",
        "\n",
        "\n",
        "#모델생성\n",
        "#실제값과 예측값의 차이를 구하는 손실함수를 구할때 class(정답)가 3개이상이면 sparse_categorical_corossentopy를 사용하고\n",
        "#class가 정답이 2개이면 categoricla_crossentoropy를 사용한다.\n",
        "#optimizer은 경사하강법을 구하는 방법을 지정한다.\n",
        "\n",
        "#model = Model(inputs =input_layer, outputs = predictions)\n",
        "model.compile(optimizer ='adam',loss ='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "#모델학습\n",
        "model.fit(train_ds, validation_data= val_ds, epochs=EPOCH, verbose= 1)\n",
        "\n",
        "\n",
        "#모델 평가\n",
        "loss, accuracy = model.evaluate(test_ds, verbose =1)\n",
        "print('accuracy: %f' %(accuracy * 100))\n",
        "print('loss: %f' % (loss))\n",
        "#모델저장\n",
        "model.save('cnn_model.h5')\n"
      ],
      "metadata": {
        "id": "jwT2C6PijqV0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "from tensorflow import keras\n",
        "from keras.models import Model, load_model\n",
        "from keras import preprocessing\n",
        "\n",
        "# 데이터 읽어오기\n",
        "train_file = \"./datasets/chatbot_data.csv\"\n",
        "data = pd.read_csv(train_file, delimiter=',')\n",
        "features = data['Q'].tolist()\n",
        "labels = data['label'].tolist()\n",
        "\n",
        "# 단어 인덱스 시퀀스 벡터\n",
        "corpus = [preprocessing.text.text_to_word_sequence(text) for text in features]\n",
        "tokenizer = preprocessing.text.Tokenizer()\n",
        "tokenizer.fit_on_texts(corpus)\n",
        "sequences = tokenizer.texts_to_sequences(corpus)\n",
        "MAX_SEQ_LEN = 15 # 단어 시퀀스 벡터 크기\n",
        "padded_seqs = preprocessing.sequence.pad_sequences(sequences, maxlen=MAX_SEQ_LEN, padding='post')\n",
        "\n",
        "# 테스트용 데이터셋 생성\n",
        "ds = tf.data.Dataset.from_tensor_slices((padded_seqs, labels))\n",
        "ds = ds.shuffle(len(features))\n",
        "test_ds = ds.take(2000).batch(20) # 테스트 데이터셋\n",
        "\n",
        "# 감정 분류 CNN 모델 불러오기\n",
        "model = load_model('cnn_model.h5')\n",
        "model.summary()\n",
        "model.evaluate(test_ds, verbose=2)\n",
        "\n",
        "# 테스트용 데이터셋의 10212번째 데이터 출력\n",
        "print(\"단어 시퀀스 : \", corpus[10212])\n",
        "print(\"단어 인덱스 시퀀스 : \", padded_seqs[10212])\n",
        "print(\"문장 분류(정답) : \", labels[10212])\n",
        "\n",
        "# 테스트용 데이터셋의 10212번째 데이터 감정 예측\n",
        "picks = [10212]\n",
        "predict = model.predict(padded_seqs[picks])\n",
        "predict_class = tf.math.argmax(predict, axis=1)\n",
        "print(\"감정 예측 점수 : \", predict)\n",
        "print(\"감정 예측 클래스 : \", predict_class.numpy())"
      ],
      "metadata": {
        "id": "_LxCGG4AkI1P"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}